{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zY6VgrWuI3_",
        "outputId": "e02eb57d-9e6c-4cc5-f8e6-a9014d54629c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ConvLab-2'...\n",
            "remote: Enumerating objects: 2621, done.\u001b[K\n",
            "remote: Counting objects: 100% (338/338), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 2621 (delta 134), reused 213 (delta 92), pack-reused 2283\u001b[K\n",
            "Receiving objects: 100% (2621/2621), 358.53 MiB | 13.45 MiB/s, done.\n",
            "Resolving deltas: 100% (1328/1328), done.\n",
            "Updating files: 100% (991/991), done.\n",
            "/content/ConvLab-2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/ConvLab-2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (3.8.1)\n",
            "Requirement already satisfied: tqdm>=4.30 in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (4.65.0)\n",
            "Collecting checksumdir>=1.1\n",
            "  Downloading checksumdir-1.2.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (8.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (0.18.3)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (1.10.1)\n",
            "Collecting scikit-learn==0.20.3\n",
            "  Downloading scikit-learn-0.20.3.tar.gz (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-pretrained-bert>=0.6.1\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<3.0.0,>=2.3.0\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (2.12.2)\n",
            "Collecting tensorboardX==1.7\n",
            "  Downloading tensorboardX-1.7-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.8.0\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides==4.1.2\n",
            "  Downloading overrides-4.1.2-py3-none-any.whl (11 kB)\n",
            "Collecting allennlp==0.9.0\n",
            "  Downloading allennlp-0.9.0-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ConvLab-2==1.0.0.dev20230426) (2.27.1)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.19.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==2.1.9\n",
            "  Downloading spacy-2.1.9.tar.gz (30.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aka334/ConvLab-2.git\n",
        "%cd ConvLab-2\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVVqKCjU7UOU",
        "outputId": "b47a40d9-7226-480f-d12c-ce9043c603b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from convlab2.nlu import NLU\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, BertForTokenClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgg6F66xuNxi",
        "outputId": "97efa45c-8104-41ed-c46a-5948067ab005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/ConvLab-2/convlab2/dialog_agent/agent.py:86: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  self.opponent_name = 'user' if self.name is 'sys' else 'sys'\n",
            "/content/ConvLab-2/convlab2/dialog_agent/agent.py:128: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if self.name is 'sys':\n",
            "/content/ConvLab-2/convlab2/dialog_agent/agent.py:146: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if self.name is 'sys':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBERTNLU(NLU):\n",
        "    def __init__(self, intent_model_path, slot_model_path, tokenizer_path, intent_label_encoder_path, slot_label_encoder_path):\n",
        "        self.intent_model = BertForSequenceClassification.from_pretrained(intent_model_path)\n",
        "        self.slot_model = BertForTokenClassification.from_pretrained(slot_model_path)\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path)\n",
        "        \n",
        "        with open(intent_label_encoder_path, 'rb') as file:\n",
        "            self.intent_label_encoder = pickle.load(file)\n",
        "            \n",
        "        with open(slot_label_encoder_path, 'rb') as file:\n",
        "            self.slot_label_encoder = pickle.load(file)\n",
        "            \n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.intent_model.to(self.device)\n",
        "        self.slot_model.to(self.device)\n",
        "        \n",
        "    def parse(self, utterance):\n",
        "        # Intent detection\n",
        "        input_tokens = self.tokenizer(utterance, return_tensors='pt', truncation=True, padding=True)\n",
        "        input_tokens = {k: v.to(self.device) for k, v in input_tokens.items()}\n",
        "        intent_logits = self.intent_model(**input_tokens).logits\n",
        "        intent_id = torch.argmax(intent_logits, dim=-1).item()\n",
        "        intent = self.intent_label_encoder.inverse_transform([intent_id])[0]\n",
        "        \n",
        "        # Slot filling\n",
        "        slot_input = self.tokenizer(utterance, return_tensors='pt', return_offsets_mapping=True, truncation=True, padding=True)\n",
        "        slot_input = {k: v.to(self.device) for k, v in slot_input.items()}\n",
        "        slot_logits = self.slot_model(**slot_input).logits\n",
        "        slot_ids = torch.argmax(slot_logits, dim=-1).squeeze(0)\n",
        "        slot_predictions = self.slot_label_encoder.inverse_transform(slot_ids.cpu())\n",
        "        slots = []\n",
        "        \n",
        "        for prediction, offset_mapping in zip(slot_predictions, slot_input['offset_mapping'][0]):\n",
        "            if prediction.startswith('B-'):\n",
        "                slot_name = prediction[2:]\n",
        "                slot_value = utterance[offset_mapping[0]: offset_mapping[1]]\n",
        "                slots.append((slot_name, slot_value))\n",
        "        \n",
        "        return intent, slots"
      ],
      "metadata": {
        "id": "lqTNFDguuvlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJL8nBN57mun",
        "outputId": "a6402b7d-b5ba-4c4e-a641-2cbe7c238de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from convlab2.dialog_agent import PipelineAgent\n",
        "from convlab2.dst.rule.multiwoz import RuleDST\n",
        "from convlab2.policy.rule.multiwoz import RulePolicy\n",
        "from convlab2.nlg.template.multiwoz import TemplateNLG\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4sy2N4B0_Pq",
        "outputId": "e8aca651-c58f-456d-992e-b99350b6e5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model_path = \"/content/drive/MyDrive/KVRET/trained_kvret_intent_model/\"\n",
        "tokenizer_path = \"/content/drive/MyDrive/KVRET/trained_kvret_intent_model/\"\n",
        "intent_label_encoder_path = \"/content/drive/MyDrive/KVRET/trained_kvret_intent_model/label_encoder.pkl\"\n",
        "slot_model_path = \"/content/drive/MyDrive/KVRET/trained_kvret_slot_model/\"\n",
        "# The tokenizer is the same for both models, so you can use the same path\n",
        "slot_label_encoder_path = \"/content/drive/MyDrive/KVRET/trained_kvret_slot_model/label_encoder.pkl\"\n"
      ],
      "metadata": {
        "id": "Qgw19-gs1FEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# config = BertConfig.from_json_file(\"/content/drive/MyDrive/KVRET/trained_kvret_intent_model/config.json\")\n"
      ],
      "metadata": {
        "id": "R9pjrRUi888o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your custom NLU component\n",
        "nlu = CustomBERTNLU(\n",
        "    intent_model_path, \n",
        "    slot_model_path, \n",
        "    tokenizer_path, \n",
        "    intent_label_encoder_path, \n",
        "    slot_label_encoder_path\n",
        ")"
      ],
      "metadata": {
        "id": "v0Olxfkw4-H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the dialogue system components\n",
        "dst = RuleDST()\n",
        "policy = RulePolicy()\n",
        "nlg = TemplateNLG(is_user=False)"
      ],
      "metadata": {
        "id": "jCOhLXb65DNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = PipelineAgent(nlu, dst, policy, nlg, name='sys')"
      ],
      "metadata": {
        "id": "tRuAhean5Ke-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can use the agent to process user inputs\n",
        "user_input = \"Can you make a system please?\"\n",
        "agent_response = agent.response(user_input)\n",
        "print(agent_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T03-QFP15Qpz",
        "outputId": "5f5112f6-202e-4b3b-a939-2815a7d75388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ' m happy to have been able to help you today .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb4C-h_UPxgv",
        "outputId": "ee36ae6b-8c84-4ed1-9dee-791752e452d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n"
      ],
      "metadata": {
        "id": "MwZq5WaZNMHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "with open(\"/content/drive/MyDrive/KVRET/dialogues.json\", \"r\") as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "FA3t6X9AOvs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dialogue(dialogue):\n",
        "    user_utterances = []\n",
        "    system_utterances = []\n",
        "    ground_truth_responses = []\n",
        "\n",
        "    for turn in dialogue[\"turns\"]:\n",
        "        if turn[\"speaker\"] == \"user\":\n",
        "            user_utterances.append(turn[\"utterance\"])\n",
        "        elif turn[\"speaker\"] == \"system\":\n",
        "            ground_truth_responses.append(turn[\"utterance\"])\n",
        "\n",
        "    return user_utterances, ground_truth_responses"
      ],
      "metadata": {
        "id": "9VUXVM6HO8h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores = []\n",
        "# f1_scores = []\n",
        "rouge_scores = []\n",
        "rouge = Rouge()\n",
        "smoothing_function = SmoothingFunction()"
      ],
      "metadata": {
        "id": "ri_NXbwAO_D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dialogue in data:\n",
        "    user_utterances, ground_truth_responses = process_dialogue(dialogue)\n",
        "    predicted_responses = []\n",
        "\n",
        "    for user_utterance in user_utterances:\n",
        "        agent_response = agent.response(user_utterance)\n",
        "        predicted_responses.append(agent_response)\n",
        "\n",
        "    # Compute BLEU scores\n",
        "    for pred, gt in zip(predicted_responses, ground_truth_responses):\n",
        "        bleu_score = sentence_bleu([gt.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function.method1)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    for pred, gt in zip(predicted_responses, ground_truth_responses):\n",
        "        if len(pred) == 0 or len(gt) == 0:\n",
        "            continue\n",
        "        rouge_score = rouge.get_scores(pred, gt, avg=True)\n",
        "        rouge_scores.append(rouge_score)\n"
      ],
      "metadata": {
        "id": "91jMJGQoPDeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROUGE scores\n",
        "for pred, gt in zip(predicted_responses, ground_truth_responses):\n",
        "    if len(pred) == 0 or len(gt) == 0:\n",
        "        continue\n",
        "    rouge_score = rouge.get_scores(pred, gt, avg=True)\n",
        "    rouge_scores.append(rouge_score)\n"
      ],
      "metadata": {
        "id": "ytWs8DGBPHJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "average_rouge_scores = {\n",
        "    metric: sum(scores[metric][\"f\"] for scores in rouge_scores) / len(rouge_scores) for metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]\n",
        "}\n",
        "\n",
        "print(\"Average BLEU score:\", average_bleu_score)\n",
        "print(\"Average ROUGE scores:\", average_rouge_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "288wJuHrQsMy",
        "outputId": "7167cfe6-4b02-482e-e865-4e624f6af797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score: 0.005800435430076993\n",
            "Average ROUGE scores: {'rouge-1': 0.05088082779550414, 'rouge-2': 0.004514755442310943, 'rouge-l': 0.04869316174412618}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    print(\"Welcome to the Interactive Dialogue System!\")\n",
        "    print(\"You can start chatting with the agent by typing a message below.\")\n",
        "    print(\"To exit the chat, type 'quit'.\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"User: \")\n",
        "        \n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        \n",
        "        # Generate agent's response using your trained model\n",
        "        agent_response = generate_response(user_input)  # Replace this function with your model's response generation method\n",
        "        \n",
        "        print(\"Agent:\", agent_response)"
      ],
      "metadata": {
        "id": "6ikUK1XMWKOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_input):\n",
        "    # Implement your model's response generation logic here\n",
        "    response =agent.response(user_input)\n",
        "    return response"
      ],
      "metadata": {
        "id": "y_0Q1MfyVp2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "80EtW53XWMpV",
        "outputId": "e32e4cfe-8d60-4c08-88bf-6c72aec34ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Interactive Dialogue System!\n",
            "You can start chatting with the agent by typing a message below.\n",
            "To exit the chat, type 'quit'.\n",
            "\n",
            "User: hello\n",
            "Agent: Thank you for using our services .\n",
            "User: I want to schedule time for 7pm\n",
            "Agent: Thank you for using our services .\n",
            "User: how are you\n",
            "Agent: Glad to have been of help . Thank you for using the Cambridge TownInfo centre . Enjoy the rest of your day !.\n",
            "User: why is this system bad\n",
            "Agent: Thank you for using our services .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e57ee30c65ef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-88cc4de15b6b>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xh9Iq4GKWOIU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}